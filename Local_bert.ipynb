{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmt/pnsoEDdY2zb+butCeZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgarvita/MISC/blob/main/Local_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In AWS instance : foldername/localGPT/privategpt/ingest_local_model\n"
      ],
      "metadata": {
        "id": "jnMejgwJUMy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install packages\n",
        "!pip install sentence_transformers\n",
        "!pip install langchain\n",
        "!pip install chroma"
      ],
      "metadata": {
        "id": "RBIvNIplr4IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model in model folder \"ggml-gpt4all-j-v1.3-groovy.bin\"\n",
        "#cell 2 - download sentence_transformer model in file embedding_load.py.\n",
        "#model then is saved in folder \"embedding_model\" in the project folder.\n"
      ],
      "metadata": {
        "id": "ZZW2VqHHUQ0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "rOK2GzHBVGXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#embedding_load.py\n",
        "from sentence_transformers import SentenceTransformer\n",
        "#emb_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "emb_model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
        "emb_model.save('sentance_transformer')\n"
      ],
      "metadata": {
        "id": "5aAMwLJvVE0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ingest_local_model.py\n",
        "#Creating embedding using embedding model and perform similarity seach of the database\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "#Using locally stored embedding model to create embeddings for the text\n",
        "#model = SentenceTransformer('/workdir/privateGPT/localgpt/privateGPT/embedding_model',device='cpu')\n",
        "model = SentenceTransformer('/workdir/privateGPT/localgpt/privateGPT/sentance_transformer',device='cpu')\n",
        "print(model)\n",
        "\n",
        "embedded_vector = model.encode(\"What is your name?\")\n",
        "print(f\"Size of embedded vector: {len(embedded_vector)}\")\n",
        "\n",
        "#Load and break the text document\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# load the document and split it into chunks\n",
        "#loader = TextLoader(\"source_documents/state_of_the_union.txt\")\n",
        "#documents = loader.load()\n",
        "\n",
        "#load pdf document\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "loader = PyMuPDFLoader(\"source_documents/Policy_Travel.pdf\")\n",
        "documents = loader.load()\n",
        "# split it into chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "#print(docs)\n",
        "\n",
        "# create the open-source embedding function\n",
        "#embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\",cache_folder='embedding_model')\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"multi-qa-mpnet-base-dot-v1\",cache_folder='sentance_transformer')\n",
        "# save the database on the disk\n",
        "db = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db\")\n",
        "db.persist()\n",
        "\n",
        "## Test the semantic retrieval\n",
        "print(db.similarity_search(query=\"Who can authorize the travel?\", k= 4))\n"
      ],
      "metadata": {
        "id": "KJzLtp5xqYE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#query.py\n",
        "#setting the dabase in retrieval mode and perform search\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from chromadb.config import Settings\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "PERSIST_DIRECTORY=\"chroma_db\"\n",
        "EMBEDDINGS_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "# Define the Chroma settings\n",
        "CHROMA_SETTINGS = Settings(\n",
        "        persist_directory=PERSIST_DIRECTORY,\n",
        "        anonymized_telemetry=False\n",
        ")\n",
        "\n",
        "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\",cache_folder='embedding_model')\n",
        "db = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embedding_function)\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "#Test the retriever\n",
        "#print(retriever.vectorstore.similarity_search(query = \"What happens if flight gets canceled?\",k=2))\n",
        "\n",
        "#Load gpt4all model\n",
        "from langchain.llms import GPT4All\n",
        "\n",
        "MODEL_N_BATCH=8\n",
        "TARGET_SOURCE_CHUNKS=4\n",
        "MODEL_PATH = \"models/ggml-gpt4all-j-v1.3-groovy.bin\"\n",
        "MODEL_N_CTX=1000\n",
        "# Prepare the LLM\n",
        "#llm = GPT4All(model=MODEL_PATH, n_ctx=MODEL_N_CTX, backend='gptj', callbacks=None, verbose=False)\n",
        "llm = GPT4All(model=MODEL_PATH, max_tokens=2000, backend='gptj', n_batch=8, callbacks=None, verbose=True)\n",
        "\n",
        "#Initialize question answer chain\n",
        "from langchain.chains import RetrievalQA\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever, return_source_documents=True)\n",
        "query = \"What happens if flight gets canceled?\"\n",
        "res = qa(query)\n",
        "answer, docs = res['result'], res['source_documents']\n",
        "\n",
        "# Get the answer from the chain\n",
        "# Print the result\n",
        "print(\"\\n\\n> Question:\")\n",
        "print(query)\n",
        "print(\"\\n> Answer:\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "-sMf7NK_rZIf",
        "outputId": "67796443-2eb9-461d-d9ea-2a8b14036dd3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a11abb10ff56>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#query.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#setting the dabase in retrieval mode and perform search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}